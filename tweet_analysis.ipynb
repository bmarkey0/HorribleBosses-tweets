{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:10:20.366552Z",
     "start_time": "2026-01-18T12:10:05.595677Z"
    }
   },
   "source": [
    "from docuscospacy import convert_corpus,frequency_table, tags_table, ngrams_by_token, ngrams_by_tag, coll_table, \\\n",
    "    tags_dtm, normalize_dtm, dtm_to_coo, kwic_center_node, keyness_table, tag_ruler\n",
    "from tmtoolkit.corpus import Corpus, vocabulary_size, corpus_num_tokens, corpus_add_tabular\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd \n",
    "import random\n"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_docusco_spacy'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men_docusco_spacy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpre_process\u001B[39m(txt):\n\u001B[1;32m     11\u001B[0m     txt \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbits\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mit s\u001B[39m\u001B[38;5;124m'\u001B[39m, txt)\n",
      "File \u001B[0;32m~/PycharmProjects/Nigerian_tweets/.venv/lib/python3.11/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Nigerian_tweets/.venv/lib/python3.11/site-packages/spacy/util.py:456\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexists\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# Path or Path-like to model data\u001B[39;00m\n\u001B[1;32m    455\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_docusco_spacy'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def csv_2_txt(inpath, outdir): #where inpath is local path to data csv, outdir to folder for split txt files\n",
    "\n",
    "    df = pd.read_csv(inpath, sep = \"\\t\", engine = \"python\", on_bad_lines= \"warn\")\n",
    "\n",
    "    df = df.drop([\"User\", \"Date Created\", \"Source of Tweet\", \"Location\", \"Number of Likes\"], axis = 1)\n",
    "\n",
    "    df[\"Tweet\"] = df[\"Tweet\"].str.lower()\n",
    "    \n",
    "    #n-grams advertising in the corpus. used to filter out advertisments. \n",
    "    ad_grams = [\"super affordable\", \"place your order\", \"giveaway\", \"asamoah gyan\", \"black stars\", \"baby jet\", \"ghanavsnigeria\", \"forextradingbot\", \"intelligence prime capital\", \"#gistlover\", \"kindly send a dm\", \"countdown\", \"stream\", \"#breakingbad\", \"#freesubscription\", \"dstv\", \"#dstv\", \"don jazzy\", \"good condition\", \"agba akin\", \"referral program\", \"#cryptocurrency\", \"multichoice\", \"#nugaunilag2022\", \"snapchat\", \"spintex\", \"#blackstars\", \"#skincareproducts\", \"#worldwaterday\", \"today's bookings\", \"#endsars\", \"oppong nkrumah\", \"for enquiries\", \"#tech tuesdays\", \"promote my business\", \"just visit the website\", \"bbnaija\", \"bbn\", \"speed darlington\", \"#sheleadsafrica\", \"daniel emeka\", \"trending health\", \"wellness gadget\", \"apple music\", \"follow back\", \"for sale\", \"#forsale\"]\n",
    "\n",
    "    df_no_ads = df[~df['Tweet'].str.contains('|'.join(ad_grams))]\n",
    "    \n",
    "    for i, value in enumerate(df_no_ads[\"Tweet\"]):\n",
    "        filename = os.path.join(f'{outdir}/{random.randint(10000, 99999)}.txt')\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(value))\n",
    "\n"
   ],
   "id": "2be17ffcc2009ef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nlp = spacy.load(\"en_docusco_spacy\")\n",
    "\n",
    "def pre_process(txt):\n",
    "    txt = re.sub(r'\\bits\\b', 'it s', txt)\n",
    "    txt = re.sub(r'\\bIts\\b', 'It s', txt)\n",
    "    txt = \" \".join(txt.split())\n",
    "    return(txt)\n",
    "\n",
    "\n",
    "def build_corpus(filepath): #where filpath is outpath from previous cell\n",
    "    \n",
    "    corpus = Corpus.from_folder(filepath,\n",
    "                              spacy_instance = nlp,\n",
    "                              raw_preproc = [pre_process],\n",
    "                              spacy_token_attrs = ['tag', 'ent_iob', 'ent_type', 'is_punct'])\n",
    "\n",
    "    return corpus     "
   ],
   "id": "7e33cc93fa9a6ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:10:06.353790Z",
     "start_time": "2024-08-25T16:10:02.588937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#this cell processes the corpus\n",
    "\n",
    "target_corpus = build_corpus(\"data/target_corpus.txt\") #replace filepath with your local path\n",
    "tar_corpus_total = corpus_num_tokens(target_corpus)\n",
    "tar_corpus_types = vocabulary_size(target_corpus)\n",
    "tar_total_punc = []\n",
    "\n",
    "for i in range(0,len(target_corpus)):\n",
    "    tar_total_punc.append(sum(target_corpus[i]['is_punct']))\n",
    "    \n",
    "tar_total_punct = sum(tar_total_punc)\n",
    "tar_non_punct = tar_corpus_total - tar_total_punct\n",
    "\n",
    "processed_target = convert_corpus(target_corpus)"
   ],
   "id": "9b8d2c324f282468",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:11:23.165563Z",
     "start_time": "2024-08-25T16:11:22.393440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#this cell contains functions for different analyses, like frequency, specific type counts, and ngrams. all functions can run with var_type 'pos' or 'ds'\n",
    "\n",
    "def get_high_freq(corpus, var_type):  \n",
    "    wc = frequency_table(corpus, count_by=var_type)\n",
    "    high_rf = wc[wc['RF'] > 100]\n",
    "    \n",
    "    return high_rf\n",
    "\n",
    "def get_tag_list(corpus, var_type):\n",
    "    dc = tags_table(corpus, count_by=var_type)\n",
    "    dc = dc.sort_values('RF', ascending = False)\n",
    "    \n",
    "    return dc\n",
    "\n",
    "def get_type_freq(corpus, type_name, var_type): #specify a corpus and a docuscope category and this function will return all tokens in that type\n",
    "    docu_wc = frequency_table(corpus, count_by=var_type)\n",
    "    type_freq_wc = docu_wc.query(f'Tag.str.starswith{type_name}')\n",
    "    \n",
    "    return type_freq_wc\n",
    "\n",
    "def get_ngrams_by_token(corpus, token, node_pos, span, var_type): #node pos = where in ngram is the token\n",
    "    \n",
    "    nc = ngrams_by_token(corpus, node_word=token, node_position=node_pos, span=span, search_type='fixed', count_by=var_type)\n",
    "\n",
    "    return nc\n"
   ],
   "id": "55b03db9d852799a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9347d81f58f43d5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
